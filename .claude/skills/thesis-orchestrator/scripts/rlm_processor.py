#!/usr/bin/env python3
"""RLM (Recursive Language Model) Processor with Chunked Pattern.

Implements chunked processing for large-scale literature review to reduce
memory usage from 150,000 tokens to 15,000 tokens (90% reduction).

Architecture:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Chunked RLM Pattern                                    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  1000 papers (150k tokens baseline)                     â”‚
    â”‚      â†“ [Chunk]                                          â”‚
    â”‚  10 chunks Ã— 100 papers each                            â”‚
    â”‚      â†“ [Process Each Chunk]                             â”‚
    â”‚  Chunk Analysis (15k tokens) â†’ Summary (1.5k tokens)    â”‚
    â”‚      â†“ [Merge Summaries]                                â”‚
    â”‚  Final Synthesis (15k tokens)                           â”‚
    â”‚      â†“                                                   â”‚
    â”‚  Peak memory: 15k tokens (vs 150k baseline)             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Usage:
    processor = RLMProcessor(
        working_dir="thesis-output/project",
        chunk_size=100
    )

    # Process large dataset
    result = processor.process_large_dataset(
        dataset=papers,  # 1000 papers
        agent_name="literature-searcher-rlm"
    )

    # Result contains:
    # - final_synthesis: 15k tokens
    # - chunk_summaries: 10 Ã— 1.5k tokens
    # - metadata: processing stats

Author: Claude Code (Thesis Orchestrator Team)
Date: 2026-01-20
"""

import sys
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict

# Import MemoryManager for RLM chunk storage
from memory_manager import MemoryManager


@dataclass
class ChunkResult:
    """Result from processing a single chunk."""

    chunk_id: int
    chunk_size: int
    chunk_analysis: str  # Full analysis (15k tokens)
    chunk_summary: str   # Compressed summary (1.5k tokens)
    processing_time: float
    memory_used: int
    timestamp: str

    def to_dict(self) -> dict:
        return asdict(self)


@dataclass
class RLMResult:
    """Final result from RLM processing."""

    agent_name: str
    total_items: int
    num_chunks: int
    chunk_results: List[ChunkResult]
    final_synthesis: str  # Final synthesis (15k tokens)
    peak_memory: int
    total_time: float
    compression_ratio: float
    timestamp: str

    def to_dict(self) -> dict:
        return asdict(self)


class RLMProcessor:
    """Chunked RLM processor for large-scale literature review."""

    DEFAULT_CHUNK_SIZE = 100  # papers per chunk
    CHUNK_ANALYSIS_MAX_TOKENS = 15000
    CHUNK_SUMMARY_MAX_TOKENS = 1500
    FINAL_SYNTHESIS_MAX_TOKENS = 15000

    def __init__(
        self,
        working_dir: Path,
        chunk_size: int = DEFAULT_CHUNK_SIZE
    ):
        """Initialize RLM processor.

        Args:
            working_dir: Project working directory
            chunk_size: Number of items per chunk (default 100)
        """
        self.working_dir = Path(working_dir)
        self.chunk_size = chunk_size

        # Memory directories
        self.memory_dir = self.working_dir / "memory"
        self.rlm_chunks_dir = self.memory_dir / "rlm-chunks"
        self.rlm_chunks_dir.mkdir(parents=True, exist_ok=True)

        # Initialize MemoryManager
        self.manager = MemoryManager(working_dir=self.working_dir)

        # Tracking
        self.current_peak_memory = 0
        self.chunk_results: List[ChunkResult] = []

    def _chunk_dataset(
        self,
        dataset: List[Any],
        chunk_size: int = None
    ) -> List[List[Any]]:
        """Split dataset into chunks.

        Args:
            dataset: List of items to chunk
            chunk_size: Size of each chunk (default: self.chunk_size)

        Returns:
            List of chunks
        """
        if chunk_size is None:
            chunk_size = self.chunk_size

        chunks = []
        for i in range(0, len(dataset), chunk_size):
            chunks.append(dataset[i:i + chunk_size])

        return chunks

    def _process_chunk(
        self,
        chunk_id: int,
        chunk: List[Any],
        agent_name: str
    ) -> ChunkResult:
        """Process a single chunk.

        This is a placeholder - in real implementation, this would call
        the actual RLM agent with the chunk data.

        Args:
            chunk_id: Chunk identifier (0-based)
            chunk: List of items in this chunk
            agent_name: Name of RLM agent (e.g., "literature-searcher-rlm")

        Returns:
            ChunkResult with analysis and summary
        """
        start_time = datetime.now()

        print(f"  [{chunk_id + 1}] Processing chunk with {len(chunk)} items...")

        # Placeholder: In real implementation, this would call RLM agent
        # For now, create a mock analysis
        chunk_analysis = self._mock_chunk_analysis(chunk_id, chunk)

        # Compress to summary (15k â†’ 1.5k tokens)
        chunk_summary = self._compress_chunk_analysis(chunk_analysis)

        # Calculate memory used
        memory_used = len(chunk_analysis.split())

        # Update peak memory
        if memory_used > self.current_peak_memory:
            self.current_peak_memory = memory_used

        processing_time = (datetime.now() - start_time).total_seconds()

        result = ChunkResult(
            chunk_id=chunk_id,
            chunk_size=len(chunk),
            chunk_analysis=chunk_analysis,
            chunk_summary=chunk_summary,
            processing_time=processing_time,
            memory_used=memory_used,
            timestamp=datetime.now().isoformat()
        )

        print(f"  [{chunk_id + 1}] âœ“ Completed in {processing_time:.2f}s")
        print(f"  [{chunk_id + 1}] âœ“ Memory: {memory_used:,} tokens")
        print(f"  [{chunk_id + 1}] âœ“ Summary: {len(chunk_summary.split())} tokens")

        return result

    def _mock_chunk_analysis(
        self,
        chunk_id: int,
        chunk: List[Any]
    ) -> str:
        """Create mock chunk analysis (placeholder).

        In real implementation, this would be the actual RLM agent output.

        Args:
            chunk_id: Chunk identifier
            chunk: Chunk data

        Returns:
            Mock analysis text
        """
        return f"""# Chunk {chunk_id + 1} Analysis

## Overview
Analyzed {len(chunk)} papers in this chunk.

## Key Findings
1. Finding 1 from chunk {chunk_id + 1}
2. Finding 2 from chunk {chunk_id + 1}
3. Finding 3 from chunk {chunk_id + 1}

## Theoretical Frameworks
- Framework A
- Framework B
- Framework C

## Empirical Evidence
- Study 1: Results showing X
- Study 2: Results showing Y
- Study 3: Results showing Z

## Gaps Identified
- Gap 1 in literature
- Gap 2 in methodology
- Gap 3 in theory

## Research Questions Suggested
- RQ1: How does X affect Y?
- RQ2: What is the relationship between A and B?
- RQ3: Why does Z occur in context C?

{'.' * 1000}  # Padding to simulate ~15k tokens analysis
"""

    def _compress_chunk_analysis(self, analysis: str) -> str:
        """Compress chunk analysis to summary.

        Compression: 15,000 â†’ 1,500 tokens (90% reduction)

        Args:
            analysis: Full chunk analysis

        Returns:
            Compressed summary
        """
        # Extract key points (placeholder - would use actual compression)
        lines = analysis.split('\n')
        key_lines = [line for line in lines if line.startswith(('#', '-', 'RQ'))]

        summary = '\n'.join(key_lines[:20])  # Top 20 key lines
        return summary

    def _merge_summaries(
        self,
        chunk_results: List[ChunkResult]
    ) -> str:
        """Merge chunk summaries into final synthesis.

        Input: 10 chunks Ã— 1,500 tokens = 15,000 tokens
        Output: Final synthesis ~15,000 tokens

        Args:
            chunk_results: List of chunk results with summaries

        Returns:
            Final synthesis
        """
        print(f"\n[Final] Merging {len(chunk_results)} chunk summaries...")

        # Collect all summaries
        all_summaries = []
        for result in chunk_results:
            all_summaries.append(f"## Chunk {result.chunk_id + 1}\n{result.chunk_summary}")

        # Create final synthesis (placeholder - would use actual merging logic)
        final_synthesis = f"""# Final Literature Review Synthesis

## Overview
Synthesized findings from {len(chunk_results)} chunks covering {sum(r.chunk_size for r in chunk_results)} total items.

## Cross-Chunk Patterns
Identified {len(chunk_results) * 3} key patterns across chunks:
- Pattern 1: Consistent across chunks 1-5
- Pattern 2: Emerging in chunks 6-10
- Pattern 3: Contradictions between chunks 3 and 7

## Integrated Findings

{''.join(all_summaries)}

## Overall Research Gaps
1. Gap A (identified in chunks 1, 3, 5, 7)
2. Gap B (identified in chunks 2, 4, 6, 8)
3. Gap C (identified in all chunks)

## Recommended Research Questions
1. RQ1: Synthesized from chunks 1-5
2. RQ2: Synthesized from chunks 6-10
3. RQ3: Cross-chunk insight

## Conclusion
{len(chunk_results)} chunks processed successfully with peak memory {self.current_peak_memory:,} tokens.
"""

        print(f"[Final] âœ“ Synthesis created: {len(final_synthesis.split())} tokens")
        return final_synthesis

    def _save_chunk_result(
        self,
        chunk_result: ChunkResult,
        agent_name: str
    ):
        """Save chunk result to RLM chunks directory.

        Args:
            chunk_result: Chunk result to save
            agent_name: Agent name
        """
        chunk_file = self.rlm_chunks_dir / f"{agent_name}_chunk_{chunk_result.chunk_id}.json"

        with open(chunk_file, 'w') as f:
            json.dump(chunk_result.to_dict(), f, indent=2)

    def _save_final_result(
        self,
        rlm_result: RLMResult,
        agent_name: str
    ):
        """Save final RLM result.

        Args:
            rlm_result: Final RLM result
            agent_name: Agent name
        """
        result_file = self.rlm_chunks_dir / f"{agent_name}_final.json"

        # Save metadata only (without full chunk analyses)
        result_dict = rlm_result.to_dict()
        result_dict['chunk_results'] = [
            {
                'chunk_id': r.chunk_id,
                'chunk_size': r.chunk_size,
                'processing_time': r.processing_time,
                'memory_used': r.memory_used,
                'timestamp': r.timestamp
                # Exclude full analysis and summary from final save
            }
            for r in rlm_result.chunk_results
        ]

        with open(result_file, 'w') as f:
            json.dump(result_dict, f, indent=2)

        # Save final synthesis separately
        synthesis_file = self.rlm_chunks_dir / f"{agent_name}_synthesis.md"
        with open(synthesis_file, 'w') as f:
            f.write(rlm_result.final_synthesis)

    def process_large_dataset(
        self,
        dataset: List[Any],
        agent_name: str,
        chunk_size: int = None
    ) -> RLMResult:
        """Process large dataset using chunked RLM pattern.

        Main entry point for RLM processing.

        Args:
            dataset: List of items to process (e.g., papers)
            agent_name: Name of RLM agent
            chunk_size: Optional chunk size override

        Returns:
            RLMResult with final synthesis and metadata
        """
        start_time = datetime.now()

        print(f"\n{'='*70}")
        print(f"ðŸ§  RLM CHUNKED PROCESSING: {agent_name}")
        print(f"{'='*70}\n")

        print(f"Dataset size: {len(dataset)} items")
        chunk_size = chunk_size or self.chunk_size
        print(f"Chunk size: {chunk_size} items")

        # 1. Chunk dataset
        print(f"\n[1/4] Chunking dataset...")
        chunks = self._chunk_dataset(dataset, chunk_size)
        print(f"  âœ“ Created {len(chunks)} chunks")

        # 2. Process each chunk
        print(f"\n[2/4] Processing chunks...")
        chunk_results = []
        for i, chunk in enumerate(chunks):
            result = self._process_chunk(i, chunk, agent_name)
            chunk_results.append(result)

            # Save chunk result immediately
            self._save_chunk_result(result, agent_name)

        print(f"  âœ“ All chunks processed")
        print(f"  âœ“ Peak memory: {self.current_peak_memory:,} tokens")

        # 3. Merge summaries
        print(f"\n[3/4] Merging chunk summaries...")
        final_synthesis = self._merge_summaries(chunk_results)

        # 4. Create final result
        print(f"\n[4/4] Finalizing result...")
        total_time = (datetime.now() - start_time).total_seconds()
        total_items = sum(r.chunk_size for r in chunk_results)

        # Calculate compression ratio
        baseline_memory = total_items * 150  # 150 tokens per item baseline
        compression_ratio = self.current_peak_memory / baseline_memory if baseline_memory > 0 else 0

        rlm_result = RLMResult(
            agent_name=agent_name,
            total_items=total_items,
            num_chunks=len(chunks),
            chunk_results=chunk_results,
            final_synthesis=final_synthesis,
            peak_memory=self.current_peak_memory,
            total_time=total_time,
            compression_ratio=compression_ratio,
            timestamp=datetime.now().isoformat()
        )

        # Save final result
        self._save_final_result(rlm_result, agent_name)

        print(f"  âœ“ Final result saved")

        print(f"\n{'='*70}")
        print(f"âœ… RLM PROCESSING COMPLETE: {agent_name}")
        print(f"   Total items: {total_items:,}")
        print(f"   Chunks: {len(chunks)}")
        print(f"   Peak memory: {self.current_peak_memory:,} tokens")
        print(f"   Baseline memory: {baseline_memory:,} tokens")
        print(f"   Reduction: {(1 - compression_ratio) * 100:.1f}%")
        print(f"   Total time: {total_time:.2f}s")
        print(f"{'='*70}\n")

        return rlm_result

    def get_processing_stats(self) -> Dict[str, Any]:
        """Get RLM processing statistics.

        Returns:
            Dictionary with processing stats
        """
        return {
            "peak_memory": self.current_peak_memory,
            "chunks_processed": len(self.chunk_results),
            "total_items": sum(r.chunk_size for r in self.chunk_results),
            "avg_chunk_time": sum(r.processing_time for r in self.chunk_results) / max(1, len(self.chunk_results))
        }


def main():
    """CLI interface for testing."""
    import argparse

    parser = argparse.ArgumentParser(
        description="RLM Chunked Processor"
    )
    parser.add_argument(
        "--working-dir",
        type=str,
        required=True,
        help="Project working directory"
    )
    parser.add_argument(
        "--dataset-size",
        type=int,
        default=1000,
        help="Size of mock dataset (default 1000)"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=100,
        help="Chunk size (default 100)"
    )
    parser.add_argument(
        "--agent-name",
        type=str,
        default="literature-searcher-rlm",
        help="Agent name"
    )

    args = parser.parse_args()

    # Create mock dataset
    mock_dataset = [{"paper_id": i, "title": f"Paper {i}"} for i in range(args.dataset_size)]

    # Process
    processor = RLMProcessor(
        working_dir=args.working_dir,
        chunk_size=args.chunk_size
    )

    result = processor.process_large_dataset(
        dataset=mock_dataset,
        agent_name=args.agent_name
    )

    # Show stats
    stats = processor.get_processing_stats()
    print("\nProcessing Statistics:")
    for key, value in stats.items():
        print(f"  {key}: {value}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
