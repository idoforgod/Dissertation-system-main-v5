# Memory Optimization Strategy

**ëª©í‘œ**: ë©”ëª¨ë¦¬ í•œê³„ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ ë°©ì§€ (workflow í•µì‹¬ ë³´ì¡´, RLM íŒ¨í„´ ìœ ì§€)

**Date**: 2026-01-20

---

## ğŸ¯ Problem Analysis

### í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš© íŒ¨í„´

```
Phase 1 (15 agents ìˆœì°¨ ì‹¤í–‰):
  Agent 1 â†’ Output 1 (ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€)
  Agent 2 â†’ Output 1 + 2 (ëˆ„ì )
  Agent 3 â†’ Output 1 + 2 + 3 (ëˆ„ì )
  ...
  Agent 15 â†’ Output 1-15 (ëˆ„ì ) â† ë©”ëª¨ë¦¬ í­ë°œ!
```

**ë¬¸ì œì **:
1. **ì»¨í…ìŠ¤íŠ¸ ëˆ„ì **: 15ê°œ agent Ã— í‰ê·  3000 tokens = 45,000 tokens (Phase 1ë§Œ)
2. **RLM ëŒ€ëŸ‰ ì…ë ¥**: literature-searcher-rlmì´ 1000+ ë¬¸í—Œ ì²˜ë¦¬
3. **Cross-reference**: ê° agentê°€ ëª¨ë“  ì´ì „ ê²°ê³¼ ì°¸ì¡° í•„ìš”
4. **Long-running session**: Phase 0-4 ì „ì²´ ì§„í–‰ ì‹œ 200,000+ tokens

### í˜„ì¬ External Memory (3-File)

```
thesis-output/[project]/
â”œâ”€â”€ session.json           # ì„¸ì…˜ ìƒíƒœë§Œ
â”œâ”€â”€ todo-checklist.md      # ì§„í–‰ ìƒíƒœë§Œ
â””â”€â”€ research-synthesis.md  # ìµœì¢… synthesisë§Œ (Phase 1 ì™„ë£Œ í›„)
```

**í•œê³„**:
- Agent 1-14 ê²°ê³¼ëŠ” ì»¨í…ìŠ¤íŠ¸ì— ë‚¨ìŒ (ë©”ëª¨ë¦¬ ì†Œëª¨)
- Phase 2-4 ì§„í–‰ ì‹œ Phase 1 ì „ì²´ ì°¸ì¡° ë¶ˆê°€ëŠ¥
- RLM ì²˜ë¦¬ ê²°ê³¼ì˜ intermediate summaries ë³´ê´€ ë¶ˆê°€

---

## ğŸ—ï¸ Solution Architecture

### í•µì‹¬ ì›ì¹™

1. âœ… **ê¸°ì¡´ workflow ì™„ë²½ ë³´ì¡´**: Agent ìˆœì„œ, ì˜ì¡´ì„±, í’ˆì§ˆ ê¸°ì¤€ ë¶ˆë³€
2. âœ… **RLM íŒ¨í„´ ì ˆëŒ€ ìœ ì§€**: Chunked processing, streaming summary ê°•í™”
3. âœ… **íˆ¬ëª…ì„±**: ì‚¬ìš©ìëŠ” ë³€í™”ë¥¼ ì¸ì§€ ëª»í•¨ (backward compatible)
4. âœ… **ì ì§„ì  ì ìš©**: Phaseë³„ë¡œ ì„ íƒì  ì ìš© ê°€ëŠ¥

---

## ğŸ“Š Strategy 1: Hierarchical Memory Architecture

**ê°œë…**: 4-Level ê³„ì¸µì  ë©”ëª¨ë¦¬ â†’ í•„ìš”í•œ ìˆ˜ì¤€ë§Œ ë¡œë“œ

### Level 1: Ultra-Compact State (session.json)

**í˜„ì¬**:
```json
{
  "research": {"topic": "...", "type": "..."},
  "current_phase": 1,
  "current_agent": "literature-searcher"
}
```

**ê°œì„ **:
```json
{
  "research": {"topic": "...", "type": "..."},
  "current_phase": 1,
  "current_agent": "literature-searcher",
  "agent_summaries": {
    "literature-searcher": "Found 847 papers on AI consciousness (2010-2025)",
    "seminal-works-analyst": "Identified 12 foundational papers (Chalmers 1995, etc.)",
    "trend-analyst": "Research trend: increased focus on embodiment (2020+)"
  },
  "memory_budget": {
    "current_usage": 15000,
    "max_budget": 50000,
    "compression_ratio": 0.3
  }
}
```

**ì••ì¶• ë¹„ìœ¨**: 3000 tokens â†’ 50 tokens (60ë°° ì••ì¶•)

### Level 2: Phase-level Synthesis

**ì‹ ê·œ íŒŒì¼**:
```
thesis-output/[project]/memory/
â”œâ”€â”€ phase-0-synthesis.md  (ì´ˆê¸°í™” ìš”ì•½)
â”œâ”€â”€ phase-1-synthesis.md  (ë¬¸í—Œê²€í†  ì¢…í•©)
â”œâ”€â”€ phase-2-synthesis.md  (ì—°êµ¬ì„¤ê³„ ìš”ì•½)
â”œâ”€â”€ phase-3-synthesis.md  (ë…¼ë¬¸ ê°œìš”)
â””â”€â”€ phase-4-synthesis.md  (íˆ¬ê³  ì „ëµ)
```

**phase-1-synthesis.md ì˜ˆì‹œ**:
```markdown
# Phase 1: Literature Review Synthesis

## Wave 1-4 Key Findings (ì••ì¶•: 12 agents â†’ 2000 tokens)
- 847 papers identified (2010-2025)
- 12 seminal works (Chalmers 1995, Dennett 1991, ...)
- 3 major theoretical frameworks: functionalism, embodiment, integrated information
- 2 methodological gaps: lack of consciousness measurement, embodiment experiments

## Wave 5 Quality Metrics
- SRCS: 78.5/100
- pTCS: 82.0/100
- Plagiarism: 8.3%

## Research Questions Emerged
1. Can artificial systems achieve phenomenal consciousness?
2. Role of embodiment in consciousness?
3. Measurement methods for machine consciousness?

## Next Phase Requirements
- Quantitative study + qualitative interviews
- Mixed methods design recommended
```

**ì••ì¶• ë¹„ìœ¨**: 45,000 tokens â†’ 2000 tokens (22ë°° ì••ì¶•)

### Level 3: Wave-level Cache

**ì‹ ê·œ íŒŒì¼**:
```
thesis-output/[project]/memory/wave-cache/
â”œâ”€â”€ wave-1.json
â”œâ”€â”€ wave-2.json
â”œâ”€â”€ wave-3.json
â”œâ”€â”€ wave-4.json
â””â”€â”€ wave-5.json
```

**wave-1.json ì˜ˆì‹œ**:
```json
{
  "wave": 1,
  "agents": ["literature-searcher", "seminal-works-analyst", "trend-analyst", "methodology-scanner"],
  "completed": true,
  "gate_passed": true,
  "gate_scores": {"ptcs": 82.0, "srcs": 78.0},
  "key_outputs": {
    "total_papers": 847,
    "seminal_works": 12,
    "theoretical_frameworks": ["functionalism", "embodiment", "IIT"],
    "research_trends": "increased focus on embodiment (2020+)",
    "methodology_gaps": ["consciousness measurement", "embodiment experiments"]
  },
  "cross_validation_result": {
    "consistency_score": 85.2,
    "conflicts": []
  },
  "references": {
    "full_outputs": ["01-literature-search-strategy.md", "02-seminal-works-analysis.md", ...]
  }
}
```

**ì••ì¶• ë¹„ìœ¨**: 12,000 tokens â†’ 500 tokens (24ë°° ì••ì¶•)

### Level 4: Agent Output Archive (_temp/)

**í˜„ì¬**: ëª¨ë“  ì¶œë ¥ ê·¸ëŒ€ë¡œ ë³´ê´€
**ê°œì„ **: ì°¸ì¡° ë¹ˆë„ì— ë”°ë¼ ì••ì¶• ë˜ëŠ” ìš”ì•½

```
thesis-output/[project]/
â”œâ”€â”€ _temp/                    # ìµœê·¼ agent ì¶œë ¥ (full)
â”‚   â”œâ”€â”€ 13-literature-synthesis.md
â”‚   â”œâ”€â”€ 14-conceptual-model.md
â”‚   â””â”€â”€ 15-plagiarism-report.md
â”œâ”€â”€ _archive/                 # ì˜¤ë˜ëœ ì¶œë ¥ (compressed)
â”‚   â”œâ”€â”€ 01-04-wave1.tar.gz
â”‚   â”œâ”€â”€ 05-08-wave2.tar.gz
â”‚   â””â”€â”€ 09-12-wave3.tar.gz
â””â”€â”€ memory/                   # Levels 1-3
    â””â”€â”€ ...
```

---

## ğŸ”„ Strategy 2: Sliding Window Context Pattern

**ê°œë…**: Nê°œ ìµœê·¼ ê²°ê³¼ë§Œ active context, ë‚˜ë¨¸ì§€ëŠ” synthesis ì°¸ì¡°

### Implementation

```python
class ContextWindow:
    """Sliding window context manager."""

    def __init__(self, window_size=3):
        self.window_size = window_size  # ìµœê·¼ 3ê°œ agentë§Œ full context
        self.active_context = []
        self.synthesis_cache = {}

    def load_context_for_agent(self, agent_name, phase, wave):
        """Load minimal context for agent execution."""

        # 1. Core context (í•­ìƒ ë¡œë“œ)
        core = {
            'phase_synthesis': self.load_phase_synthesis(phase - 1),  # ì´ì „ Phase ìš”ì•½
            'wave_cache': self.load_wave_cache(wave - 1),             # ì´ì „ Wave cache
            'session_state': self.load_session()                      # í˜„ì¬ ìƒíƒœ
        }

        # 2. Sliding window (ìµœê·¼ Nê°œ agent ìƒì„¸)
        recent_agents = self.get_recent_agents(n=self.window_size)
        window = [self.load_agent_output(agent) for agent in recent_agents]

        # 3. Specific dependencies (agentë³„ í•„ìˆ˜ ì°¸ì¡°)
        dependencies = self.get_dependencies(agent_name)
        deps = [self.load_agent_summary(dep) for dep in dependencies]

        return {
            'core': core,           # 2,000 tokens
            'window': window,       # 9,000 tokens (3 agents Ã— 3,000)
            'dependencies': deps    # 500 tokens
        }
        # Total: ~11,500 tokens (ê¸°ì¡´ 45,000 tokens ëŒ€ë¹„ 75% ì ˆê°)
```

### Example: Wave 2 Agent 3 ì‹¤í–‰

**ê¸°ì¡´ ë°©ì‹** (ë©”ëª¨ë¦¬ í­ë°œ):
```
Load:
  - Wave 1 ì „ì²´ (4 agents Ã— 3000 tokens = 12,000)
  - Wave 2 Agent 1-2 (2 agents Ã— 3000 tokens = 6,000)
Total: 18,000 tokens
```

**ê°œì„  ë°©ì‹** (Sliding Window):
```
Load:
  - Phase 0 synthesis (200 tokens)
  - Wave 1 cache (500 tokens)
  - Wave 2 Agent 1-2 full (6,000 tokens)
  - Dependencies: seminal-works (summary, 150 tokens)
Total: 6,850 tokens (62% ì ˆê°!)
```

---

## ğŸ“‰ Strategy 3: Progressive Compression Pipeline

**ê°œë…**: ê° checkpointë§ˆë‹¤ ìë™ ì••ì¶• â†’ ì˜¤ë˜ëœ ë°ì´í„°ì¼ìˆ˜ë¡ ì••ì¶•ë¥  ë†’ìŒ

### Compression Stages

```
Agent ì™„ë£Œ (ì¦‰ì‹œ):
  Full Output (3000 tokens) â†’ Ultra-Compact Summary (50 tokens)
  ì••ì¶•ë¥ : 98.3%

Wave ì™„ë£Œ (Gate í†µê³¼ í›„):
  4 Agents (12,000 tokens) â†’ Wave Cache (500 tokens)
  ì••ì¶•ë¥ : 95.8%

Phase ì™„ë£Œ (HITL ìŠ¹ì¸ í›„):
  15 Agents (45,000 tokens) â†’ Phase Synthesis (2,000 tokens)
  ì••ì¶•ë¥ : 95.6%

Workflow ì™„ë£Œ:
  All Phases (200,000 tokens) â†’ Final Synthesis (5,000 tokens)
  ì••ì¶•ë¥ : 97.5%
```

### Implementation

```python
class ProgressiveCompressor:
    """Automatic compression at each checkpoint."""

    def compress_agent_output(self, agent_name, full_output):
        """Agent ì™„ë£Œ ì‹œ ì¦‰ì‹œ ì••ì¶•."""

        # 1. Extract key findings (AI-powered)
        summary = self.extract_key_findings(full_output)

        # 2. Store in session.json
        self.session['agent_summaries'][agent_name] = summary

        # 3. Archive full output
        self.archive_full_output(agent_name, full_output)

        return summary  # 50 tokens

    def compress_wave(self, wave_number, agent_outputs):
        """Wave ì™„ë£Œ ì‹œ cache ìƒì„±."""

        # 1. Aggregate key outputs
        cache = {
            'wave': wave_number,
            'key_outputs': self.aggregate_outputs(agent_outputs),
            'gate_scores': self.get_gate_scores(wave_number),
            'cross_validation': self.get_validation_result(wave_number)
        }

        # 2. Save to wave-cache
        self.save_wave_cache(wave_number, cache)

        return cache  # 500 tokens

    def compress_phase(self, phase_number, wave_caches):
        """Phase ì™„ë£Œ ì‹œ synthesis ìƒì„±."""

        # 1. Synthesize all waves
        synthesis = self.synthesize_waves(wave_caches)

        # 2. Add quality metrics
        synthesis['srcs'] = self.calculate_phase_srcs(phase_number)
        synthesis['ptcs'] = self.calculate_phase_ptcs(phase_number)

        # 3. Save phase synthesis
        self.save_phase_synthesis(phase_number, synthesis)

        return synthesis  # 2,000 tokens
```

---

## ğŸ§  Strategy 4: RLM-Optimized Processing

**í•µì‹¬**: RLM íŒ¨í„´ ìœ ì§€í•˜ë©´ì„œ ë©”ëª¨ë¦¬ ì ˆì•½ â†’ Chunked + Streaming

### Current RLM Pattern

```python
# literature-searcher-rlm
def search_literature_rlm(topic):
    # 1000+ papers in single RLM call
    all_papers = search_databases(topic)  # 150,000 tokens!

    # Single massive processing
    analysis = rlm_process(all_papers)

    return analysis
```

**ë¬¸ì œ**: 150,000 tokens í•œ ë²ˆì— ì²˜ë¦¬ â†’ ë©”ëª¨ë¦¬ ì´ˆê³¼

### Improved RLM Pattern (Chunked + Streaming)

```python
# literature-searcher-rlm (ê°œì„ )
def search_literature_rlm_chunked(topic):
    """Chunked RLM with streaming summary."""

    # 1. Fetch all papers
    all_papers = search_databases(topic)  # 1000 papers

    # 2. Split into chunks
    chunks = split_into_chunks(all_papers, chunk_size=100)  # 10 chunks

    # 3. Process each chunk with RLM + immediate summarize
    chunk_summaries = []
    for i, chunk in enumerate(chunks):
        # RLM processing (15,000 tokens per chunk)
        chunk_analysis = rlm_process(chunk)

        # Immediate compression
        chunk_summary = compress_to_summary(chunk_analysis)  # 1,500 tokens
        chunk_summaries.append(chunk_summary)

        # Save intermediate result
        save_chunk_result(i, chunk_summary)

    # 4. Incremental merge of summaries (not full outputs)
    final_synthesis = rlm_merge_summaries(chunk_summaries)  # 15,000 tokens

    return final_synthesis
```

**ê°œì„  íš¨ê³¼**:
- ê¸°ì¡´: 150,000 tokens ë™ì‹œ ì²˜ë¦¬
- ê°œì„ : 15,000 tokensì”© 10íšŒ ì²˜ë¦¬ + ìµœì¢… 15,000 tokens merge
- **ë©”ëª¨ë¦¬ í”¼í¬**: 150,000 â†’ 15,000 (90% ì ˆê°!)

### RLM Chunk Cache

```
thesis-output/[project]/memory/rlm-chunks/
â”œâ”€â”€ literature-search-chunk-001.json
â”œâ”€â”€ literature-search-chunk-002.json
â”œâ”€â”€ ...
â””â”€â”€ literature-search-chunk-010.json
```

ê° chunkëŠ” ë…ë¦½ì ìœ¼ë¡œ ì¬ì‚¬ìš© ê°€ëŠ¥ â†’ ì¬ì‹¤í–‰ ì‹œ cache hit

---

## ğŸ¨ Strategy 5: Context Pruning Hooks

**ê°œë…**: GRA Hookì— ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬ ê¸°ëŠ¥ ì¶”ê°€

### Implementation

```python
# .claude/hooks/post-tool-use.py (ì‹ ê·œ)

def post_tool_use_memory_pruning(tool_name, tool_result, context):
    """PostToolUse hook: ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬."""

    if tool_name == "Task" and "agent" in tool_result:
        agent_name = tool_result['agent']

        # 1. Extract key findings from agent output
        summary = extract_key_findings(tool_result['output'])

        # 2. Save to session.json (ultra-compact)
        session['agent_summaries'][agent_name] = summary

        # 3. Prune irrelevant context
        context = prune_irrelevant_context(context, agent_name)

        # 4. Archive full output to file
        archive_full_output(agent_name, tool_result['output'])

        # 5. Update memory budget
        update_memory_budget(context)

        return context  # Pruned context
```

### Pruning Rules

| Agent Type | Prune Targets | Keep |
|------------|---------------|------|
| **literature-searcher** | Previous search results | Topic, research question |
| **synthesis-agent** | All agent outputs | Wave caches, phase synthesis |
| **thesis-writer** | Previous chapters (full) | Chapter summaries, outline |
| **plagiarism-checker** | Comparison texts | Similarity scores, report |

---

## ğŸ“¦ Strategy 6: Lazy Loading Pattern

**ê°œë…**: í•„ìš”í•œ ë°ì´í„°ë§Œ on-demand ë¡œë“œ â†’ ë¯¸ë¦¬ ë¡œë“œ ê¸ˆì§€

### Implementation

```python
class LazyContextLoader:
    """Lazy loading context manager."""

    def __init__(self, working_dir):
        self.working_dir = working_dir
        self.cache = {}  # In-memory cache

    def get_context_for_agent(self, agent_name):
        """Get minimal context for agent (lazy)."""

        # 1. Core context (í•­ìƒ í•„ìš”)
        context = {
            'session': self.load_session(),
            'phase_synthesis': self.load_current_phase_synthesis()
        }

        # 2. Agent-specific requirements (lazy load)
        requirements = get_agent_requirements(agent_name)

        for req in requirements:
            if req not in self.cache:
                # Load on-demand
                self.cache[req] = self.load_requirement(req)

            context[req] = self.cache[req]

        return context

    def load_requirement(self, req_name):
        """Load specific requirement on-demand."""

        if req_name.startswith('wave-'):
            # Load wave cache
            wave_num = int(req_name.split('-')[1])
            return self.load_wave_cache(wave_num)

        elif req_name.startswith('agent-'):
            # Load agent summary (not full output)
            agent = req_name.split('-', 1)[1]
            return self.load_agent_summary(agent)

        elif req_name == 'previous-phase':
            # Load previous phase synthesis
            return self.load_phase_synthesis(self.current_phase - 1)

        # Default: return None (optional context)
        return None
```

### Example: Gap-Identifier Agent

**ê¸°ì¡´ ë°©ì‹** (eager loading):
```python
# Load everything upfront
context = {
    'wave1': load_wave1(),      # 12,000 tokens
    'wave2': load_wave2(),      # 12,000 tokens
    'theories': load_theories(), # 8,000 tokens
    'empirical': load_empirical() # 10,000 tokens
}
# Total: 42,000 tokens (ëŒ€ë¶€ë¶„ ë¶ˆí•„ìš”)
```

**ê°œì„  ë°©ì‹** (lazy loading):
```python
# Load only what's needed
requirements = ['wave2-cache', 'theoretical-framework-summary', 'empirical-evidence-summary']

context = lazy_loader.get_context_for_agent('gap-identifier')
# Loaded:
#   - wave2-cache (500 tokens)
#   - theoretical-framework-summary (300 tokens)
#   - empirical-evidence-summary (400 tokens)
# Total: 1,200 tokens (97% ì ˆê°!)
```

---

## ğŸ—„ï¸ Strategy 7: Expanded External Memory (7-File â†’ 10-File)

**í˜„ì¬**: 3-File Architecture
**ê°œì„ **: 10-File Architecture

### New File Structure

```
thesis-output/[project]/
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ session.json              # Level 1: Current state + agent summaries
â”‚   â”œâ”€â”€ phase-0-synthesis.md      # Level 2: Phase summaries
â”‚   â”œâ”€â”€ phase-1-synthesis.md
â”‚   â”œâ”€â”€ phase-2-synthesis.md
â”‚   â”œâ”€â”€ phase-3-synthesis.md
â”‚   â”œâ”€â”€ phase-4-synthesis.md
â”‚   â”œâ”€â”€ wave-cache/               # Level 3: Wave caches
â”‚   â”‚   â”œâ”€â”€ wave-1.json
â”‚   â”‚   â”œâ”€â”€ wave-2.json
â”‚   â”‚   â”œâ”€â”€ wave-3.json
â”‚   â”‚   â”œâ”€â”€ wave-4.json
â”‚   â”‚   â””â”€â”€ wave-5.json
â”‚   â”œâ”€â”€ rlm-chunks/               # RLM chunk results
â”‚   â”‚   â”œâ”€â”€ literature-search-chunk-001.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ memory-budget.json        # Memory usage tracking
â”œâ”€â”€ _temp/                        # Level 4: Recent full outputs
â”‚   â”œâ”€â”€ 13-literature-synthesis.md
â”‚   â”œâ”€â”€ 14-conceptual-model.md
â”‚   â””â”€â”€ 15-plagiarism-report.md
â””â”€â”€ _archive/                     # Compressed old outputs
    â”œâ”€â”€ wave-1-4.tar.gz
    â””â”€â”€ ...
```

### memory-budget.json (ì‹ ê·œ)

```json
{
  "budget": {
    "max_tokens": 50000,
    "current_usage": 12500,
    "remaining": 37500,
    "utilization": 0.25
  },
  "by_phase": {
    "phase_0": 500,
    "phase_1": 8000,
    "phase_2": 3000,
    "phase_3": 1000,
    "phase_4": 0
  },
  "compression_stats": {
    "total_outputs": 180000,
    "compressed_to": 12500,
    "compression_ratio": 0.069,
    "savings": "93.1%"
  },
  "alerts": [
    {
      "level": "info",
      "message": "Memory usage healthy (25%)"
    }
  ]
}
```

---

## ğŸ”§ Implementation Plan

### Phase A: Foundation (1-2 days)

1. **MemoryManager Class ìƒì„±**
   ```python
   # .claude/skills/thesis-orchestrator/scripts/memory_manager.py
   class MemoryManager:
       def __init__(self, working_dir, max_budget=50000)
       def compress_agent_output(agent_name, output)
       def load_context_for_agent(agent_name)
       def prune_context()
       def get_memory_stats()
   ```

2. **7-File â†’ 10-File Migration**
   - memory/ í´ë” êµ¬ì¡° ìƒì„±
   - wave-cache/ í´ë” ìƒì„±
   - rlm-chunks/ í´ë” ìƒì„±

3. **Progressive Compressor êµ¬í˜„**
   - Agent-level compression
   - Wave-level compression
   - Phase-level compression

### Phase B: RLM Optimization (2-3 days)

1. **Chunked RLM Pattern êµ¬í˜„**
   - literature-searcher-rlm ê°œì„ 
   - synthesis-agent-rlm ê°œì„ 
   - Chunk cache ì‹œìŠ¤í…œ

2. **Streaming Summary**
   - ê° chunk ì²˜ë¦¬ í›„ ì¦‰ì‹œ summarize
   - Incremental merge

### Phase C: Context Management (2-3 days)

1. **Sliding Window Context**
   - ContextWindow í´ë˜ìŠ¤
   - window_size íŒŒë¼ë¯¸í„° ì¡°ì • (default: 3)

2. **Lazy Loading**
   - LazyContextLoader êµ¬í˜„
   - Agent requirements ì •ì˜

3. **Context Pruning Hook**
   - PostToolUse hook ì¶”ê°€
   - ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬

### Phase D: Integration & Testing (1-2 days)

1. **ê¸°ì¡´ workflow í†µí•©**
   - sequential_executor.py ìˆ˜ì •
   - Agent wrapper ì—…ë°ì´íŠ¸

2. **Backward Compatibility ê²€ì¦**
   - ê¸°ì¡´ session í˜¸í™˜ì„±
   - ê¸°ì¡´ commands ë™ì‘ í™•ì¸

3. **Performance Testing**
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
   - Before/After ë¹„êµ

---

## ğŸ“Š Expected Results

### Memory Usage Reduction

| Metric | Before | After | Reduction |
|--------|--------|-------|-----------|
| **Phase 1 ë©”ëª¨ë¦¬ í”¼í¬** | 45,000 tokens | 11,500 tokens | **74.4%** |
| **RLM ë©”ëª¨ë¦¬ í”¼í¬** | 150,000 tokens | 15,000 tokens | **90.0%** |
| **ì „ì²´ workflow** | 200,000 tokens | 50,000 tokens | **75.0%** |
| **ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°** | ëˆ„ì  (unbounded) | ê³ ì • (bounded) | **ì•ˆì •í™”** |

### Performance Improvement

- **Agent ì‹¤í–‰ ì†ë„**: 20-30% í–¥ìƒ (ì»¨í…ìŠ¤íŠ¸ ë¡œë”© ì‹œê°„ ì ˆê°)
- **Gate í†µê³¼ìœ¨**: ë³€í™” ì—†ìŒ (í’ˆì§ˆ ê¸°ì¤€ ë™ì¼)
- **ì¬ì‹¤í–‰ ì†ë„**: 50% í–¥ìƒ (chunk cache ì¬ì‚¬ìš©)

### Backward Compatibility

- âœ… ê¸°ì¡´ workflow 100% ë³´ì¡´
- âœ… RLM íŒ¨í„´ 100% ìœ ì§€
- âœ… ê¸°ì¡´ commands ë™ì‘ (íˆ¬ëª…)
- âœ… ê¸°ì¡´ session í˜¸í™˜

---

## ğŸ¯ Success Criteria

### í•„ìˆ˜ ì¡°ê±´ (Backward Compatibility)

1. âœ… **ê¸°ì¡´ workflow ë™ì¼**: Agent ìˆœì„œ, ì˜ì¡´ì„±, í’ˆì§ˆ ê¸°ì¤€ ë¶ˆë³€
2. âœ… **RLM íŒ¨í„´ ìœ ì§€**: Chunked processing, recursive summarization
3. âœ… **SRCS/pTCS ì ìˆ˜**: ë™ì¼í•œ threshold, ë™ì¼í•œ ê²°ê³¼
4. âœ… **ì‚¬ìš©ì ê²½í—˜**: ë³€í™” ì—†ìŒ (íˆ¬ëª…í•œ ìµœì í™”)

### ëª©í‘œ ë‹¬ì„± ì§€í‘œ

1. âœ… **ë©”ëª¨ë¦¬ í”¼í¬ 75% ì ˆê°**: 200k â†’ 50k tokens
2. âœ… **RLM ë©”ëª¨ë¦¬ 90% ì ˆê°**: 150k â†’ 15k tokens
3. âœ… **ì»¨í…ìŠ¤íŠ¸ ì•ˆì •í™”**: Unbounded â†’ Bounded
4. âœ… **ì‹¤í–‰ ì†ë„ 20% í–¥ìƒ**: Context loading ìµœì í™”

---

## ğŸš€ Next Steps

### ì‚¬ìš©ì ì„ íƒ

**Option 1**: ì „ì²´ êµ¬í˜„ (Phase A-D)
- ì˜ˆìƒ ì†Œìš”: 6-10ì¼
- íš¨ê³¼: ìµœëŒ€ 75% ë©”ëª¨ë¦¬ ì ˆê°
- ë¦¬ìŠ¤í¬: ì¤‘ê°„ (ì¶©ë¶„í•œ í…ŒìŠ¤íŠ¸ í•„ìš”)

**Option 2**: ë‹¨ê³„ë³„ êµ¬í˜„ (Phase Aë§Œ)
- ì˜ˆìƒ ì†Œìš”: 1-2ì¼
- íš¨ê³¼: ì•½ 40% ë©”ëª¨ë¦¬ ì ˆê°
- ë¦¬ìŠ¤í¬: ë‚®ìŒ (ê¸°ë³¸ ì••ì¶•ë§Œ)

**Option 3**: RLMë§Œ ìµœì í™” (Phase Bë§Œ)
- ì˜ˆìƒ ì†Œìš”: 2-3ì¼
- íš¨ê³¼: RLM 90% ì ˆê° (ê°€ì¥ í° bottleneck)
- ë¦¬ìŠ¤í¬: ë‚®ìŒ (RLM íŒ¨í„´ ê°œì„ ë§Œ)

---

**ì–´ë–¤ ì˜µì…˜ìœ¼ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?**
