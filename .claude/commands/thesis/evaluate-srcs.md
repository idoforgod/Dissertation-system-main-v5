---
description: SRCS 4ì¶• í‰ê°€ (Citation, Grounding, Uncertainty, Verifiability)
context: fork
agent: general-purpose
---

# SRCS í‰ê°€

SRCS (Structured Research Claim Score) 4ì¶• í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

## ì—­í• 

ì´ ì»¤ë§¨ë“œëŠ” **SRCS 4ì¶• ì¢…í•© í‰ê°€**ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:
- CS (Citation Score): ì¶œì²˜ í’ˆì§ˆ
- GS (Grounding Score): ê·¼ê±° í’ˆì§ˆ
- US (Uncertainty Score): ë¶ˆí™•ì‹¤ì„± í‘œí˜„
- VS (Verifiability Score): ê²€ì¦ê°€ëŠ¥ì„±

## ì „ì œ ì¡°ê±´

- í‰ê°€í•  ë¬¸í—Œê²€í†  ë˜ëŠ” ë…¼ë¬¸ íŒŒì¼ ì¡´ì¬
- GroundedClaim ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜ íŒŒì¼

## ì‹¤í–‰ ë°©ë²•

```python
import sys
import json
from pathlib import Path

# Add scripts to path
sys.path.insert(0, str(Path.cwd() / ".claude" / "skills" / "thesis-orchestrator" / "scripts"))

from cross_validator import extract_claims_from_file
from srcs_evaluator import (
    evaluate_all_claims,
    generate_summary,
    generate_quality_report,
    DEFAULT_THRESHOLD,
)

# Get working directory from session.json
session_file = Path("thesis-output") / "session.json"
if session_file.exists():
    with open(session_file) as f:
        session = json.load(f)
    working_dir = Path(session["working_dir"])
else:
    print("âŒ Error: No active session found. Run /thesis:init first.")
    sys.exit(1)

# Find all GroundedClaim files in literature directory
lit_dir = working_dir / "01-literature"
claim_files = sorted(lit_dir.glob("wave*.md")) if lit_dir.exists() else []

if not claim_files:
    # Fallback to _temp directory
    temp_dir = working_dir / "_temp"
    claim_files = list(temp_dir.glob("*.md"))

if not claim_files:
    print("âŒ Error: No claim files found")
    sys.exit(1)

# Extract claims from all files
all_claims = []
for claim_file in claim_files:
    claims = extract_claims_from_file(str(claim_file))
    all_claims.extend(claims)

# Evaluate all claims (returns a dict with overall_scores, evaluated_claims, etc.)
result = evaluate_all_claims(all_claims)

# Extract overall scores from result dict
overall_scores = result.get("overall_scores", {})
cs_avg = overall_scores.get("cs", 0)
gs_avg = overall_scores.get("gs", 0)
us_avg = overall_scores.get("us", 0)
vs_avg = overall_scores.get("vs", 0)
overall = overall_scores.get("total", 0)
grade = result.get("grade", "F")

# Print results
print("\n" + "="*70)
print("           SRCS EVALUATION RESULTS")
print("="*70)
print(f"\nTotal Claims Evaluated: {result.get('total_claims', 0)}")
print(f"\nğŸ“Š SRCS Scores:")
print(f"  CS (Citation):      {cs_avg:.1f}/100")
print(f"  GS (Grounding):     {gs_avg:.1f}/100")
print(f"  US (Uncertainty):   {us_avg:.1f}/100")
print(f"  VS (Verifiability): {vs_avg:.1f}/100")
print(f"\nğŸ¯ Overall SRCS:      {overall:.1f}/100")
print(f"   Grade:             {grade}")
print(f"   Pass Rate:         {result.get('pass_rate', 0)}%")

# Threshold check
threshold = DEFAULT_THRESHOLD
if overall >= threshold:
    print(f"\nâœ… PASSED: SRCS ({overall:.1f}) meets threshold ({threshold})")
else:
    print(f"\nâŒ FAILED: SRCS ({overall:.1f}) below threshold ({threshold})")

# Save JSON summary report
report_file = working_dir / "srcs-evaluation-report.json"
generate_summary(result, report_file)

# Save markdown quality report
report_md = working_dir / "quality-report.md"
generate_quality_report(result, report_md)

print(f"\nğŸ“„ JSON report: {report_file}")
print(f"ğŸ“„ Quality report: {report_md}")
print("="*70)
```

## ì¶œë ¥

```
thesis-output/[project]/
â”œâ”€â”€ srcs-evaluation-report.json
â””â”€â”€ quality-report.md
```

## SRCS ë“±ê¸‰ ê¸°ì¤€

| Grade | Score Range | Description |
|-------|-------------|-------------|
| A+ | 90-100 | Outstanding |
| A | 85-89 | Excellent |
| B+ | 80-84 | Very Good |
| B | 75-79 | Good (Pass) |
| C | 70-74 | Acceptable (Caution) |
| D | 60-69 | Poor (Fail) |
| F | 0-59 | Unacceptable (Fail) |

**Threshold: 75ì  ì´ìƒ í•„ìˆ˜**

## ê´€ë ¨ ëª…ë ¹ì–´

- `/thesis:validate-phase` - Phase ê²€ì¦
- `/thesis:check-plagiarism` - í‘œì ˆ ê²€ì‚¬
- `/thesis:run-literature-review` - ë¬¸í—Œê²€í†  ì‹¤í–‰

$ARGUMENTS
